{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before you begin\n",
    "\n",
    "1. Setup **gcloud** by following this guide [here](https://29022131.atlassian.net/wiki/spaces/DP/pages/1006174505/JupyterHub+-+End-user+Guide#JupyterHub-End-userGuide-GCloudsetup).\n",
    "2. Setup **github** by following this guide [here](https://29022131.atlassian.net/wiki/spaces/DP/pages/1006174505/JupyterHub+-+End-user+Guide#JupyterHub-End-userGuide-Githubsetup).\n",
    "3. Setup **rs-sdk** and know how to store and get your secret key (no bare key allowed) by following this guide [here](https://29022131.atlassian.net/wiki/spaces/DATA/pages/1010736425/RS-SDK+User+Guide)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PySpark to access S3 and securely use secret key through rs-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window, SparkSession\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import rs-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rs.keys import GateKeeper\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Environment Variable for Service Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/absolute/path/to/service-account.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize GateKeeper Object to manage your secret stored in the a cloud storage within GCP Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gk = GateKeeper('tvlk-my-project-id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the encrypted secret using rs-sdk\n",
    "Refer to this guide [here](https://29022131.atlassian.net/wiki/spaces/DATA/pages/1010736425/RS-SDK+User+Guide)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "acckey = gk.get_secret(keyring_id=\"my-keyring\", key_id=\"my-key\", storage_bucket=\"tvlk-mybucket\", file_path=\"mykey/my_access_key_id.enc\").decode('utf-8')\n",
    "accsecret = gk.get_secret(keyring_id=\"my-keyring\", key_id=\"my-key\", storage_bucket=\"tvlk-mybucket\", file_path=\"mykey/my_secret_access_key.enc\").decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Locate your resources in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeBucket = \"tvlk-data-mybucket\"\n",
    "myprefix = \"my-playground\"\n",
    "filename = \"00/part*\"\n",
    "sample_data_path = \"s3a://%s:%s@%s/%s/%s\" % (acckey, accsecret, writeBucket, myprefix, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiliaze SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName(\"sample-workload\")\\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data from S3\n",
    "Here we set example how to load Avro data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = spark\\\n",
    "                    .read\\\n",
    "                    .format(\"avro\")\\\n",
    "                    .load(sample_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do whatever ETL logic you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = (\n",
    "        sample_data\n",
    "        .where(sf.col('auth_level') == 'IDENTIFIED')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_row = filtered_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of the filtered data are:\n",
      "11169\n",
      "it is supposed to be 11169\n"
     ]
    }
   ],
   "source": [
    "print(\"the length of the filtered data are:\")\n",
    "print(count_row)\n",
    "print(\"it is supposed to be 11169\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docs",
   "language": "python",
   "name": "docs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
