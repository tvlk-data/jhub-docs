{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before your begin\n",
    "\n",
    "1. Setup **gcloud** by following this guide [here](https://29022131.atlassian.net/wiki/spaces/DP/pages/1006174505/JupyterHub+-+End-user+Guide#JupyterHub-End-userGuide-GCloudsetup).\n",
    "2. Setup **github** by following this guide [here](https://29022131.atlassian.net/wiki/spaces/DP/pages/1006174505/JupyterHub+-+End-user+Guide#JupyterHub-End-userGuide-Githubsetup)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export BQ data to GCS to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, a convenient way to convert BQ query result to pandas dataframe is to use to_dataframe() method. However, if your data is large, it will become significantly slower.\n",
    "One way to tackle this problem is to export the query result to another BQ temp table, then export this table to GCS in CSV format, then load it to pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate bq client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client(project=\"YOUR-PROJECT-ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "        \"SELECT \"\n",
    "        \"trip_distance, fare_amount \"\n",
    "        \"FROM \"\n",
    "        \"`nyc-tlc.yellow.trips` \"\n",
    "        \"WHERE \"\n",
    "        \"trip_distance > 0 \"\n",
    "        \"AND passenger_count > 0 \"\n",
    "        \"AND fare_amount >= 2.5 \"\n",
    "        \"AND pickup_longitude > -78 \"\n",
    "        \"AND pickup_longitude < -70 \"\n",
    "        \"AND dropoff_longitude > -78 \"\n",
    "        \"AND dropoff_longitude < -70 \"\n",
    "        \"AND pickup_latitude > 37 \"\n",
    "        \"AND pickup_latitude < 45 \"\n",
    "        \"AND dropoff_latitude > 37 \"\n",
    "        \"AND dropoff_latitude < 45 \"\n",
    "        \"AND MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))), 5000) = 1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate bq job config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"YOUR-DATASET-NAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"YOUR-TEMP-TABLE-NAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config = bigquery.QueryJobConfig()\n",
    "table_ref = client.dataset(dataset_name).table(table_name)\n",
    "job_config.destination = table_ref\n",
    "# by default it will truncate the table if exists\n",
    "job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute the query and save the result to another BQ temp table (configured in the job_config above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_job = client.query(\n",
    "    query,\n",
    "    location=\"US\",\n",
    "    job_config=job_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the table to a CSV file in GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_uri = \"gs://YOUR-BUCKET/path/to/your/file.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_job = client.extract_table(\n",
    "    table_ref,\n",
    "    destination_uri,\n",
    "    location='US')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load CSV file in GCS to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gcsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(destination_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deleting BQ temp table and GCS file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_table(table_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(\"YOUR-BUCKET\")\n",
    "blob = bucket.blob(\"path/to/your/file.csv\")\n",
    "\n",
    "blob.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "andri",
   "language": "python",
   "name": "andri"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
